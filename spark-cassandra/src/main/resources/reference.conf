######################################
# Spark Cassandra Reference Config File #
######################################

spark-cassandra {

  spark {
    # The Spark master host.
    master = "127.0.0.1"
    port = 7077
    app-name  = "demo app"
    load-jars = [
      "/var/lib/cassandra/cassandra-driver-spark_2.10-1.0.0-SNAPSHOT.jar"
      ]
    # loads defaults from java system properties and the classpath
    # TODO false for units
    load-defaults = true
  }

  # cassandra.yaml settings
  cassandra {
    name = "MyDemoCluster"
    keyspace = "test_keyspace"

    # See https://github.com/datastax/spark-cassandra-connector/blob/master/doc/1_connecting.md
    # for the default settings.
    connection {
      # The contact point to connect to the Cassandra cluster.
      # Defaults to spark master host.
      seed-nodes = ["127.0.0.1"]
      # Cassandra thrift port.
      rpc-port = 9160
      # Cassandra native port.
      native-port = 9042

      # Optional for tuning #

      # The duration to keep unused connections open.
      keep-alive = 100 ms
      # The number of times to retry a failed query.
      retry-count = 10

      # The delay durations for determining the frequency to try to reconnect to a downed node.
      reconnect-delay {
        # The initial delay.
        min = 1000 ms
        # The final delay.
        max = 60000 s
      }

      # TODO from env: user pass
      auth {
        # The login for password authentication.
        username = "cassandra"
        # The password for password authentication.
        password = "cassandra"
        # The name of the class implementing [[AuthConfFactory]] for custom authentication.
        auth-impl = "DefaultAuthConfFactory"
      }
    }

    # To reduce the number of roundtrips to Cassandra, partitions are paged
    input {
      # The approx number of rows in a Spark partition. Defaults to 1000.
      paged-row-count = 1000
      # The number of rows to fetch in a single Spark Task. Defaults to 100000.
      task-row-count = 100000
    }

    output {

      # The maximum number of batches executed in parallel by a single Spark task.
      # Defaults to 5.
      concurrent-writes = 5

      # The number of rows per single batch. Defaults to 'auto', i.e. the driver
      # will adjust the number of rows based on the amount of data in each row.
      row-size = "auto"
      # The maximum total size of the batch in bytes. Defaults to 64 kB.
      max-bytes = 64kb
    }
  }
}

akka {

}