######################################
# Spark Cassandra Akka Extension Reference Config File #
######################################

spark-cassandra {

  spark {
    # not using yet, but one could eventually use this extension just for cassandra without spark
    enabled = true

    # The Spark master host. Environment override if exists.
    # Options:
    # - "local" to run locally with one thread,
    # - local[4]" to run locally with 4 cores
    # - the master IP/hostname to run on a Spark standalone cluster
    # Supports optional HA failover for more than one: host1,host2..
    # which is used to inform: spark://host1:port1,host2:port2
    master = [${?SPARK_MASTER_IP}]
    port = 7077
    app-name  = "demo app"
    load-jars = [
      "/var/lib/cassandra/spark-cassandra-connector_2.10-1.0.0-beta1.jar"
      ]
    # loads defaults from java system properties and the classpath
    # TODO false option for units
    load-defaults = true
    cores-max = 10
  }

  cassandra {
    # not using yet, but one could eventually use this extension just for spark without cassandra
    enabled = true
    name = "MyDemoCluster"
    keyspace = "test_keyspace"

    # See https://github.com/datastax/spark-cassandra-connector/blob/master/doc/1_connecting.md
    # for the default settings.
    connection {
      # The contact point to connect to the Cassandra cluster.
      # Defaults to spark master host.
      seed-nodes = ["127.0.0.1"]
      # Cassandra thrift port.
      rpc-port = 9160
      # Cassandra native port.
      native-port = 9042

      # Optional for tuning #

      # The duration to keep unused connections open.
      keep-alive = 100 ms
      # The number of times to retry a failed query.
      retry-count = 10

      # The delay durations for determining the frequency to try to reconnect to a downed node.
      reconnect-delay {
        # The initial delay.
        min = 1000 ms
        # The final delay.
        max = 60000 s
      }

      # Auth: These are expected to be set in the env by chef, etc.
      # - optional settings if exists in the environment
      # - falls back to optional java system props if exists
      #  - final fallback if nothing else is set
      auth {
        # Override with java -Dspark-cassandra.cassandra.connection.auth.username=fu
        # Defaults to 'cassandra'
        username = ${?CASSANDRA_USERNAME}
        # Override with java -Dspark-cassandra.cassandra.connection.auth.password=bar
        # Defaults to 'cassandra'
        password = ${?CASSANDRA_PASSWORD}
        # The name of the class implementing [[AuthConfFactory]] for custom authentication.
        auth-impl = "com.datastax.spark.connector.cql.DefaultAuthConfFactory"
      }
    }

    # To reduce the number of roundtrips to Cassandra, partitions are paged
    input {
      # The approx number of rows in a Spark partition. Defaults to 1000.
      paged-row-count = 1000
      # The number of rows to fetch in a single Spark Task. Defaults to 100000.
      task-row-count = 100000
    }

    output {

      # The maximum number of batches executed in parallel by a single Spark task.
      # Defaults to 5.
      concurrent-writes = 5

      # The number of rows per single batch. Defaults to 'auto', i.e. the driver
      # will adjust the number of rows based on the amount of data in each row.
      row-size = "auto"
      # The maximum total size of the batch in bytes. Defaults to 64 kB.
      max-bytes = 64kb
    }
  }
}

akka {

  loglevel = "INFO"

  actor {
    # "akka.cluster.ClusterActorRefProvider"
    provider = "akka.remote.RemoteActorRefProvider"
    debug {
      receive = off
      event-stream = off
    }
  }

  remote {
    netty {
      tcp.port = 0
      message-frame-size = 5 MiB
    }
    # The default is on
    log-remote-lifecycle-events = off
    log-received-messages = off
    log-sent-messages = off
  }
}